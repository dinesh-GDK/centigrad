{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43c574c",
   "metadata": {},
   "source": [
    "# centigrad demo using MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bdc7b",
   "metadata": {},
   "source": [
    "### Install additional dependencies\n",
    "To install `centigrad` dependencies see `requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install idx2numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf1fb3",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd23509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from urllib import request\n",
    "\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from centigrad.tensor import Tensor\n",
    "from centigrad.layer import (\n",
    "    Flatten,\n",
    "    FullyConnected,\n",
    "    Conv2d,\n",
    "    MaxPool2d,\n",
    "    Dropout2d,\n",
    "    BatchNorm2d,\n",
    ")\n",
    "from centigrad.optimizer import GradientDescent\n",
    "from centigrad.loss import cross_entropy\n",
    "from centigrad.activation import relu, softmax, tanh\n",
    "from centigrad.model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40717b3d",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d1e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "\n",
    "dataset = [\n",
    "    \"train-images.idx3-ubyte\",\n",
    "    \"train-labels.idx1-ubyte\",\n",
    "    \"t10k-images.idx3-ubyte\",\n",
    "    \"t10k-labels.idx1-ubyte\",\n",
    "]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for file in dataset:\n",
    "    _file = file.replace(\".\", \"-\") + \".gz\"\n",
    "\n",
    "    if _file not in os.listdir():\n",
    "        with request.urlopen(f\"{base_url}{_file}\") as response, open(\n",
    "            f\"data/{_file}\", \"wb\"\n",
    "        ) as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "\n",
    "    if file not in os.listdir():\n",
    "        with gzip.open(f\"data/{_file}\", \"rb\") as f_in, open(\n",
    "            f\"data/{file}\", \"wb\"\n",
    "        ) as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d889ca9",
   "metadata": {},
   "source": [
    "### Function to one-hot-code the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3c255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_onehot(index, num_of_classes=10):\n",
    "    label = np.zeros((index.shape[0], num_of_classes))\n",
    "    for i in range(index.shape[0]):\n",
    "        label[i, index[i]] = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00170bd",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e6b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and normalize\n",
    "train_images = idx2numpy.convert_from_file(\"../data/\" + dataset[0]) / 255.0\n",
    "test_images = idx2numpy.convert_from_file(\"../data/\" + dataset[2]) / 255.0\n",
    "\n",
    "# the dimension of the inputs should be BxCxHxW (batch x channel x height x width)\n",
    "train_images = np.expand_dims(train_images, axis=1)\n",
    "test_images = np.expand_dims(test_images, axis=1)\n",
    "\n",
    "# load the labels\n",
    "train_labels = idx2numpy.convert_from_file(\"../data/\" + dataset[1])\n",
    "test_labels = idx2numpy.convert_from_file(\"../data/\" + dataset[3])\n",
    "\n",
    "# one-hot-encode the labels\n",
    "train_labels = label_to_onehot(train_labels)\n",
    "test_labels = label_to_onehot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5581b",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "Here this model showcases all the layers and activation functions available\n",
    "(except tanh activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb698f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNet(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layerc1 = Conv2d(1, 2)\n",
    "        self.maxpool = MaxPool2d()\n",
    "        self.dropout = Dropout2d()\n",
    "        self.batchnorm = BatchNorm2d(2)\n",
    "        self.flatten = Flatten()\n",
    "        self.layer1 = FullyConnected(338, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = relu(self.layerc1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.flatten(x)\n",
    "        x = softmax(self.layer1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de84ed",
   "metadata": {},
   "source": [
    "### Define the parameters for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164d455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "net = MnistNet()\n",
    "optimizer = GradientDescent(net.parameters(), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeea2ef",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f393a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 1/10: 100%|█████████████████████| 1875/1875 [00:56<00:00, Loss=0.569323]\n",
      "Epochs: 2/10: 100%|█████████████████████| 1875/1875 [00:56<00:00, Loss=0.366532]\n",
      "Epochs: 3/10: 100%|█████████████████████| 1875/1875 [00:56<00:00, Loss=0.336350]\n",
      "Epochs: 4/10: 100%|█████████████████████| 1875/1875 [00:57<00:00, Loss=0.320589]\n",
      "Epochs: 5/10: 100%|█████████████████████| 1875/1875 [00:56<00:00, Loss=0.313759]\n",
      "Epochs: 6/10: 100%|█████████████████████| 1875/1875 [00:56<00:00, Loss=0.308322]\n",
      "Epochs: 7/10: 100%|█████████████████████| 1875/1875 [00:56<00:00, Loss=0.306048]\n",
      "Epochs: 8/10: 100%|█████████████████████| 1875/1875 [00:56<00:00, Loss=0.298902]\n",
      "Epochs: 9/10: 100%|█████████████████████| 1875/1875 [00:56<00:00, Loss=0.294765]\n",
      "Epochs: 10/10: 100%|████████████████████| 1875/1875 [00:56<00:00, Loss=0.296286]\n"
     ]
    }
   ],
   "source": [
    "b = \"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}{postfix}]\"\n",
    "\n",
    "net.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    r_loss = 0\n",
    "\n",
    "    # calculate number of batches\n",
    "    no_batches = len(train_images) // batch_size + (\n",
    "        1 if len(train_images) % batch_size != 0 else 0\n",
    "    )\n",
    "\n",
    "    pbar = tqdm(\n",
    "        total=no_batches, ncols=80, bar_format=b, desc=f\"Epochs: {e+1}/{epochs}\"\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(train_images), batch_size):\n",
    "        image, label = Tensor(train_images[i : i + batch_size]), Tensor(\n",
    "            train_labels[i : i + batch_size]\n",
    "        )\n",
    "\n",
    "        x = net(image)\n",
    "        loss = cross_entropy(x, label)\n",
    "        loss.backward()\n",
    "\n",
    "        r_loss += loss.item().mean()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(Loss=f\"{(r_loss/(i+1)):04f}\")\n",
    "\n",
    "    pbar.set_postfix(Loss=f\"{(r_loss/no_batches):04f}\")\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42465f16",
   "metadata": {},
   "source": [
    "### Save the model usng pickle if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f259368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"centigrad_model.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(net, outfile)\n",
    "\n",
    "del net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d01436",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25afdf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.32%\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "with open(\"centigrad_model.pickle\", \"rb\") as outfile:\n",
    "    net = pickle.load(outfile)\n",
    "\n",
    "net.inference()\n",
    "acc = 0\n",
    "for i in range(0, len(test_images), batch_size):\n",
    "    images, labels = Tensor(test_images[i : i + batch_size]), Tensor(\n",
    "        test_labels[i : i + batch_size]\n",
    "    )\n",
    "    inference = net(images)\n",
    "\n",
    "    for predict, label in zip(\n",
    "        np.argmax(inference.data, axis=-1), np.argmax(labels.data, axis=-1)\n",
    "    ):\n",
    "        if predict == label:\n",
    "            acc += 1\n",
    "\n",
    "print(f\"Accuracy: {(100*acc/len(test_images)):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
